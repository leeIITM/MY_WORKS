{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOcZj+IcKvuGZCwdCJ6O7YG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leeIITM/MY_WORKS/blob/main/vit_yolov4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StMwRhUnVK89",
        "outputId": "92062317-8a7c-44d2-e5c9-2b1d42a9339c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Pascal VOC 2007 dataset...\n",
            "Download complete.\n",
            "Extracting dataset...\n",
            "Extraction complete.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import tarfile\n",
        "\n",
        "# Directory to save dataset\n",
        "dataset_dir = 'VOCdevkit/'\n",
        "\n",
        "# URL for Pascal VOC 2007\n",
        "voc2007_url = 'http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar'\n",
        "\n",
        "# Create directory if not exists\n",
        "if not os.path.exists(dataset_dir):\n",
        "    os.makedirs(dataset_dir)\n",
        "\n",
        "\n",
        "# Download and extract Pascal VOC 2007\n",
        "voc_tar = os.path.join(dataset_dir, 'VOCtrainval_06-Nov-2007.tar')\n",
        "if not os.path.exists(voc_tar):\n",
        "    print(\"Downloading Pascal VOC 2007 dataset...\")\n",
        "    urllib.request.urlretrieve(voc2007_url, voc_tar)\n",
        "    print(\"Download complete.\")\n",
        "\n",
        "# Extract the dataset\n",
        "if not os.path.exists(os.path.join(dataset_dir, 'VOC2007')):\n",
        "    print(\"Extracting dataset...\")\n",
        "    with tarfile.open(voc_tar) as tar:\n",
        "        tar.extractall(path=dataset_dir)\n",
        "    print(\"Extraction complete.\")\n",
        "else:\n",
        "    print(\"Dataset already extracted.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_dir)\n",
        "dataset_dir = \"/content/VOCdevkit/VOCdevkit/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qA3XlXufVxZ7",
        "outputId": "bf276893-4027-44c0-83cb-4f7a03dcb78f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VOCdevkit/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Class mapping for VOC dataset\n",
        "class_map = {\n",
        "    \"aeroplane\": 0,\n",
        "    \"bicycle\": 1,\n",
        "    \"bird\": 2,\n",
        "    \"boat\": 3,\n",
        "    \"bottle\": 4,\n",
        "    \"bus\": 5,\n",
        "    \"car\": 6,\n",
        "    \"cat\": 7,\n",
        "    \"chair\": 8,\n",
        "    \"cow\": 9,\n",
        "    \"diningtable\": 10,\n",
        "    \"dog\": 11,\n",
        "    \"horse\": 12,\n",
        "    \"motorbike\": 13,\n",
        "    \"person\": 14,\n",
        "    \"pottedplant\": 15,\n",
        "    \"sheep\": 16,\n",
        "    \"sofa\": 17,\n",
        "    \"train\": 18,\n",
        "    \"tvmonitor\": 19,\n",
        "}\n"
      ],
      "metadata": {
        "id": "ZCCMe5LiOg0S"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Define a custom collate function\n",
        "def collate_fn(batch):\n",
        "    images, boxes, labels = zip(*batch)\n",
        "    images = torch.stack(images, dim=0)  # Stack images into a tensor\n",
        "    return images, boxes, labels  # Return boxes and labels as lists\n",
        "\n",
        "# VOCDataset with image and annotation processing\n",
        "class VOCDataset(Dataset):\n",
        "    def __init__(self, dataset_dir, image_set='trainval', transform=None):\n",
        "        super(VOCDataset, self).__init__()\n",
        "        self.dataset_dir = dataset_dir\n",
        "        self.image_set = image_set\n",
        "        self.transform = transform if transform else transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        self.image_ids = self._load_image_set()\n",
        "\n",
        "    def _load_image_set(self):\n",
        "        split_file = os.path.join(self.dataset_dir, 'VOC2007', 'ImageSets', 'Main', f'{self.image_set}.txt')\n",
        "        with open(split_file, 'r') as f:\n",
        "            image_ids = [line.strip() for line in f]\n",
        "        return image_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_id = self.image_ids[index]\n",
        "\n",
        "        # Loading the image\n",
        "        image_path = os.path.join(self.dataset_dir, 'VOC2007', 'JPEGImages', f'{image_id}.jpg')\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "        # Loading annotations\n",
        "        annotation_path = os.path.join(self.dataset_dir, 'VOC2007', 'Annotations', f'{image_id}.xml')\n",
        "        boxes, labels = self._load_annotation(annotation_path)\n",
        "\n",
        "        # Converting image to tensor and resize\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, boxes, labels\n",
        "\n",
        "    def _load_annotation(self, annotation_path):\n",
        "    # Parse the XML file to extract bounding box info and labels\n",
        "      tree = ET.parse(annotation_path)\n",
        "      root = tree.getroot()\n",
        "\n",
        "      boxes = []\n",
        "      labels = []\n",
        "      for obj in root.findall('object'):\n",
        "          bbox = obj.find('bndbox')\n",
        "          xmin = int(bbox.find('xmin').text)\n",
        "          ymin = int(bbox.find('ymin').text)\n",
        "          xmax = int(bbox.find('xmax').text)\n",
        "          ymax = int(bbox.find('ymax').text)\n",
        "          boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "          label = obj.find('name').text\n",
        "          if label in class_map:\n",
        "              labels.append(class_map[label])\n",
        "          else:\n",
        "              print(f\"Warning: Invalid class '{label}' in {annotation_path}\")\n",
        "              labels.append(-1)  # or handle it appropriately\n",
        "\n",
        "      return boxes, labels\n",
        "\n",
        "# Correct dataset path\n",
        "dataset = VOCDataset(dataset_dir='/content/VOCdevkit/VOCdevkit')\n",
        "\n",
        "# DataLoader to load data in batches using the custom collate function\n",
        "data_loader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Test loading a batch of data\n",
        "for images, boxes, labels in data_loader:\n",
        "\n",
        "    print(f\"Images shape: {images.shape}\")\n",
        "    print(f\"Boxes: {boxes}\")\n",
        "    print(f\"Labels: {labels}\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY9esN53V2an",
        "outputId": "10f7c89f-1064-4e6c-b7bb-2c652b50982e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Boxes: ([[52, 55, 479, 371], [109, 39, 178, 64], [254, 43, 270, 56], [220, 43, 236, 54]], [[4, 164, 164, 303], [99, 169, 281, 303], [214, 173, 355, 283], [286, 175, 433, 285], [375, 180, 484, 272]], [[2, 29, 195, 500], [130, 181, 347, 500]], [[3, 1, 365, 500]], [[302, 129, 449, 227], [93, 156, 288, 312], [89, 204, 116, 262]], [[40, 124, 97, 255], [92, 214, 122, 252], [58, 95, 202, 299], [175, 113, 281, 293], [203, 95, 487, 375], [7, 143, 61, 273], [372, 180, 500, 375]], [[15, 3, 476, 344], [97, 115, 443, 344]], [[488, 276, 500, 320], [475, 251, 486, 273], [279, 225, 478, 340], [245, 230, 316, 280], [1, 241, 87, 375], [133, 244, 184, 367], [121, 273, 213, 367]])\n",
            "Labels: ([6, 6, 6, 6], [5, 5, 5, 5, 5], [14, 14], [14], [17, 17, 15], [12, 12, 12, 12, 12, 14, 14], [14, 8], [6, 6, 6, 6, 6, 14, 13])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from timm import create_model\n",
        "\n",
        "class ViTBackbone(nn.Module):\n",
        "    def __init__(self, model_name='vit_base_patch16_224', pretrained=True):\n",
        "        super(ViTBackbone, self).__init__()\n",
        "        self.vit = create_model(model_name, pretrained=pretrained, num_classes=0)  # No final classification layer\n",
        "        self.patch_size = 16\n",
        "        self.out_channels = 768\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        _, _, H, W = x.shape\n",
        "        assert H % self.patch_size == 0 and W % self.patch_size == 0, \"Input size must be divisible by patch size\"\n",
        "\n",
        "        # Extracting features using Vision Transformer\n",
        "        features = self.vit(x)  # Shape: [batch_size, num_patches, feature_dim]\n",
        "        #print(f\"ViT features shape: {features.shape}\")\n",
        "\n",
        "        if features.ndim == 2:  # Shape: [batch_size, feature_dim]\n",
        "            B, C = features.shape\n",
        "            grid_size = int(H / self.patch_size)\n",
        "            # Reshape to [batch_size, channels, height, width]\n",
        "            features = features.reshape(B, C, 1, 1).expand(B, C, grid_size, grid_size)\n",
        "        else:\n",
        "            B, N, C = features.shape  # B=batch_size, N=num_patches, C=feature_dim\n",
        "            grid_size = int(H / self.patch_size)\n",
        "            features = features.reshape(B, grid_size, grid_size, C).permute(0, 3, 1, 2)\n",
        "\n",
        "        #print(f\"ViT reshaped features shape: {features.shape}\")\n",
        "        return features\n",
        "\n",
        "# YOLOv4 Neck (without SPP) bcoz i faced some errors with SPP\n",
        "class YOLOv4Neck(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(YOLOv4Neck, self).__init__()\n",
        "\n",
        "        # Simple PANet-like neck without SPP\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply simple PANet-style convolution layers\n",
        "        x = self.conv1(x)\n",
        "        #print(f\"Neck output shape after conv1: {x.shape}\")\n",
        "        x = self.conv2(x)\n",
        "        #print(f\"Neck output shape after conv2: {x.shape}\")\n",
        "\n",
        "        return x\n",
        "\n",
        "# YOLOv4 Head (Final Detection Layer)\n",
        "class YOLOv4Head(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes, num_anchors):\n",
        "        super(YOLOv4Head, self).__init__()\n",
        "\n",
        "        # Final detection head with convolutional layers for bounding box prediction\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels * 2, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels * 2, num_anchors * (5 + num_classes), kernel_size=1)  # BBox coords, obj score, classes\n",
        "        self.pool = nn.AdaptiveAvgPool2d((7, 7))  # Ensure the output is [batch_size, channels, 7, 7]\n",
        "\n",
        "    def forward(self, x, num_anchors, num_classes):\n",
        "        x = self.conv1(x)\n",
        "        #print(f\"Head output shape after conv1: {x.shape}\")\n",
        "        x = self.conv2(x)\n",
        "        #print(f\"Head output shape after conv2: {x.shape}\")\n",
        "        x = self.pool(x)\n",
        "        #print(f\"Head output shape after pool: {x.shape}\")\n",
        "\n",
        "        # Reshape the output to match the anchors and classes\n",
        "        batch_size = x.size(0)\n",
        "        output_reshaped = x.view(batch_size, num_anchors, 5 + num_classes, 7, 7).permute(0, 3, 4, 1, 2)\n",
        "        #print(num_anchors)\n",
        "        # Now output_reshaped has the shape [batch_size, 7, 7, num_anchors, 5 + num_classes]\n",
        "        output_final = output_reshaped.contiguous().view(batch_size, 7, 7, (5 + num_classes) * num_anchors)\n",
        "        return output_final\n",
        "\n",
        "# Full YOLOv4 with ViT Backbone\n",
        "class YOLOv4ViT(nn.Module):\n",
        "    def __init__(self, num_classes, num_anchors=3):\n",
        "        super(YOLOv4ViT, self).__init__()\n",
        "\n",
        "        # Vision Transformer as the backbone\n",
        "        self.backbone = ViTBackbone()\n",
        "\n",
        "        # YOLOv4 Neck (No SPP here)\n",
        "        self.neck = YOLOv4Neck(in_channels=self.backbone.out_channels, out_channels=256)\n",
        "\n",
        "        # YOLOv4 Head (Detection head with anchors)\n",
        "        self.head = YOLOv4Head(in_channels=256, num_classes=num_classes, num_anchors=num_anchors)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #Extracting features from the image using ViT backbone\n",
        "        features = self.backbone(x)\n",
        "\n",
        "        #Passing features through the neck (PANet)\n",
        "        neck_output = self.neck(features)\n",
        "\n",
        "        #Passing neck output to the YOLOv4 head for final detection\n",
        "        detection_output = self.head(neck_output, num_anchors=3, num_classes=20)\n",
        "\n",
        "        return detection_output\n",
        "\n",
        "# Define a simple loss function (e.g., combination of classification and bounding box loss)\n",
        "class YOLOLoss(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(YOLOLoss, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        # Predictions and targets shape: [batch_size, num_anchors * (5 + num_classes), H, W]\n",
        "        # The first 4 values in the last dimension correspond to bounding box coordinates\n",
        "        # The 5th value is the objectness score, followed by class scores\n",
        "\n",
        "        # Calculate loss (you can refine this for your specific loss function)\n",
        "        loss = 0.0\n",
        "        # For simplicity, use a placeholder loss (e.g., Mean Squared Error)\n",
        "        loss += nn.MSELoss()(predictions, targets)  # Replace with your detailed loss calculations\n",
        "\n",
        "        return loss\n",
        "\n",
        "# Instantiate the model\n",
        "num_classes = 20  # Number of classes in the Pascal VOC dataset\n",
        "model = YOLOv4ViT(num_classes=num_classes)\n",
        "\n",
        "# Define the loss function\n",
        "criterion = YOLOLoss(num_classes)\n",
        "\n",
        "# Set up the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust learning rate as necessary\n",
        "\n",
        "# Move loss function and optimizer to the appropriate device (if you're using GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "criterion.to(device)\n",
        "\n",
        "# Print a summary of the optimizer\n",
        "print(optimizer)"
      ],
      "metadata": {
        "id": "c7nQMQ3qWx1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4OTeeQKMJtz",
        "outputId": "20ae3ef9-e30c-4ffb-9942-5680376102b2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLOv4ViT(\n",
            "  (backbone): ViTBackbone(\n",
            "    (vit): VisionTransformer(\n",
            "      (patch_embed): PatchEmbed(\n",
            "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "        (norm): Identity()\n",
            "      )\n",
            "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "      (patch_drop): Identity()\n",
            "      (norm_pre): Identity()\n",
            "      (blocks): Sequential(\n",
            "        (0): Block(\n",
            "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): Attention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (q_norm): Identity()\n",
            "            (k_norm): Identity()\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls1): Identity()\n",
            "          (drop_path1): Identity()\n",
            "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls2): Identity()\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (1): Block(\n",
            "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): Attention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (q_norm): Identity()\n",
            "            (k_norm): Identity()\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls1): Identity()\n",
            "          (drop_path1): Identity()\n",
            "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls2): Identity()\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (2): Block(\n",
            "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): Attention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (q_norm): Identity()\n",
            "            (k_norm): Identity()\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls1): Identity()\n",
            "          (drop_path1): Identity()\n",
            "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls2): Identity()\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (3): Block(\n",
            "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): Attention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (q_norm): Identity()\n",
            "            (k_norm): Identity()\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls1): Identity()\n",
            "          (drop_path1): Identity()\n",
            "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls2): Identity()\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (4): Block(\n",
            "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): Attention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (q_norm): Identity()\n",
            "            (k_norm): Identity()\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls1): Identity()\n",
            "          (drop_path1): Identity()\n",
            "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls2): Identity()\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (5): Block(\n",
            "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): Attention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (q_norm): Identity()\n",
            "            (k_norm): Identity()\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls1): Identity()\n",
            "          (drop_path1): Identity()\n",
            "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls2): Identity()\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (6): Block(\n",
            "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): Attention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (q_norm): Identity()\n",
            "            (k_norm): Identity()\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls1): Identity()\n",
            "          (drop_path1): Identity()\n",
            "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls2): Identity()\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (7): Block(\n",
            "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): Attention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (q_norm): Identity()\n",
            "            (k_norm): Identity()\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls1): Identity()\n",
            "          (drop_path1): Identity()\n",
            "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls2): Identity()\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (8): Block(\n",
            "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): Attention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (q_norm): Identity()\n",
            "            (k_norm): Identity()\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls1): Identity()\n",
            "          (drop_path1): Identity()\n",
            "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls2): Identity()\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (9): Block(\n",
            "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): Attention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (q_norm): Identity()\n",
            "            (k_norm): Identity()\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls1): Identity()\n",
            "          (drop_path1): Identity()\n",
            "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls2): Identity()\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (10): Block(\n",
            "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): Attention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (q_norm): Identity()\n",
            "            (k_norm): Identity()\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls1): Identity()\n",
            "          (drop_path1): Identity()\n",
            "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls2): Identity()\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (11): Block(\n",
            "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): Attention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (q_norm): Identity()\n",
            "            (k_norm): Identity()\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls1): Identity()\n",
            "          (drop_path1): Identity()\n",
            "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls2): Identity()\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "      )\n",
            "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (fc_norm): Identity()\n",
            "      (head_drop): Dropout(p=0.0, inplace=False)\n",
            "      (head): Identity()\n",
            "    )\n",
            "  )\n",
            "  (neck): YOLOv4Neck(\n",
            "    (conv1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (head): YOLOv4Head(\n",
            "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(512, 75, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (pool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_targets(boxes, labels, num_classes, img_size, grid_size):\n",
        "    targets = torch.zeros((len(boxes), grid_size, grid_size, 3*(5 + num_classes)))\n",
        "\n",
        "    for i, (box_list, label_list) in enumerate(zip(boxes, labels)):\n",
        "        #print(f\"Processing box list: {box_list}\")\n",
        "        if not isinstance(box_list, list) or len(box_list) == 0:\n",
        "            print(f\"Skipping invalid box: {box_list}\")\n",
        "            continue\n",
        "\n",
        "        for box, label in zip(box_list, label_list):\n",
        "            if len(box) != 4:\n",
        "                print(f\"Skipping invalid box: {box}\")\n",
        "                continue\n",
        "\n",
        "            # Convert label to integer if it is a string\n",
        "            if isinstance(label, str):\n",
        "                try:\n",
        "                    label = int(label)\n",
        "                except ValueError:\n",
        "                    print(f\"Skipping invalid label: {label}\")\n",
        "                    continue\n",
        "\n",
        "            # Calculate box center, width, and height in normalized coordinates\n",
        "            center_x = (box[0] + box[2]) / 2  # x_min + x_max\n",
        "            center_y = (box[1] + box[3]) / 2  # y_min + y_max\n",
        "            box_width = box[2] - box[0]\n",
        "            box_height = box[3] - box[1]\n",
        "\n",
        "            # Calculate grid cell location and clamp values to avoid out-of-bounds indexing\n",
        "            grid_x = min(int(center_x * grid_size / img_size[0]), grid_size - 1)\n",
        "            grid_y = min(int(center_y * grid_size / img_size[1]), grid_size - 1)\n",
        "\n",
        "            # Assign box parameters to the targets tensor\n",
        "            targets[i, grid_y, grid_x, 0] = 1  # Objectness score\n",
        "            targets[i, grid_y, grid_x, 1:5] = torch.tensor([center_x / img_size[0], center_y / img_size[1], box_width / img_size[0], box_height / img_size[1]])  # Box coordinates normalized\n",
        "            targets[i, grid_y, grid_x, 5 + label] = 1  # One-hot encoding of the class label\n",
        "\n",
        "    return targets\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2bIG2Cj7V_39"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, data_loader, criterion, optimizer, num_epochs, device):\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "\n",
        "        for images, boxes, labels in data_loader:\n",
        "            images = images.to(device)  # Move images to the GPU\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Prepare targets\n",
        "            img_size = (images.shape[2], images.shape[3])  # (height, width) of the input image\n",
        "            grid_size = 7  # Adjust this based on the grid size in your YOLO model\n",
        "\n",
        "            targets = format_targets(boxes, labels, num_classes, img_size, grid_size)\n",
        "            #print(\"target tensor\")\n",
        "            #print(targets.shape)\n",
        "            #print(\"output tensor\")\n",
        "            #print(outputs.shape)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "OqFdjPXxWERx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_anchors = 3\n",
        "num_classes = 20\n",
        "model = YOLOv4ViT(num_classes=num_classes).to(device)\n",
        "criterion = YOLOLoss(num_classes=num_classes)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "HydQrDBjWH7f"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "train_model(model, data_loader, criterion, optimizer, num_epochs=10, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKRmugwuWKcx",
        "outputId": "eff9f33c-723a-4178-ee6e-cb627a40570d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Epoch [1/10], Loss: 0.0032\n",
            "Epoch [2/10], Loss: 0.0070\n"
          ]
        }
      ]
    }
  ]
}